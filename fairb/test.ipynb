{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import re\n",
    "from filelock import FileLock\n",
    "from datetime import datetime\n",
    "\n",
    "import datalad.api as dl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'sub-001A_bet'\n",
    "job_id = '1111'\n",
    "status_csv = '/misc/geminis2/ramirezd/fb_test/code/status.csv'\n",
    "status_lockfile = '/misc/geminis2/ramirezd/fb_test/code/status_lockfile'\n",
    "super_ds_id = '64f1d8ac-346d-4a0a-8c5c-8dca7e25ef7c'\n",
    "clone_target = '/misc/geminis2/ramirezd/test_bet/.fairlybig/input_ria/'\n",
    "push_target = '/misc/geminis2/ramirezd/test_bet/.fairlybig/output_ria/'\n",
    "push_lockfile = '/misc/geminis2/ramirezd/fb_test/code/push_lockfile'\n",
    "inputs = 'inputs/mri-raw/sub-001A/anat/sub-001A_T1w.nii.gz'\n",
    "outputs = 'outputs/sub-001A_T1w_bet.nii.gz'\n",
    "output_datasets = ['outputs']\n",
    "preget_inputs = None\n",
    "is_explicit = False\n",
    "dl_cmd = 'bet inputs/mri-raw/sub-001A/anat/sub-001A_T1w.nii.gz outputs/sub-001A_T1w_bet.nii.gz'\n",
    "commit = None\n",
    "container = 'fsl-6-0-4'\n",
    "message = None\n",
    "ephemeral_locations = ['/tmp', '/misc/{host}[0-9]/ramirezd']\n",
    "req_disk_gb = 40\n",
    "\n",
    "host = os.uname().nodename\n",
    "user= os.getenv('USER')\n",
    "\n",
    "status_lock = FileLock(status_lockfile)\n",
    "push_lock = FileLock(push_lockfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/misc/geminis2/ramirezd/fb_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for disk space management\n",
    "def get_locations(location_list):\n",
    "    \"\"\"\n",
    "    Return tmp and non_tmp locations from the list of location patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    tmp=[]\n",
    "    not_tmp_patterns=[]\n",
    "    not_tmp_locations=[]\n",
    "\n",
    "    # tmp and non_tmp list\n",
    "    for location in location_list:\n",
    "        if location == '/tmp' or location == '/tmp/':\n",
    "            tmp.append(location)\n",
    "        else:\n",
    "            not_tmp_patterns.append(location)\n",
    "    \n",
    "    # get non_tmp locations according to the non_tmp_patterns (the script can accept multiple location patterns)\n",
    "    for not_tmp_pattern in not_tmp_patterns:\n",
    "        \n",
    "        for index, part in enumerate(Path(not_tmp_pattern).parts):\n",
    "            if host in part:\n",
    "                break\n",
    "\n",
    "        # node location\n",
    "        mount_pattern = str(Path(*list(Path(not_tmp_pattern).parts[:index+1])))\n",
    "        # location inside node\n",
    "        after_pattern = str(Path(*list(Path(not_tmp_pattern).parts[index+1:])))\n",
    "        \n",
    "        # make sure those locations are within the node with the /etc/mtab file\n",
    "        with open('/etc/mtab', 'r') as mtab:\n",
    "            for line in mtab.readlines():\n",
    "                # which mount pattern is within the node\n",
    "                pattern = re.search(f'{mount_pattern} ', line)\n",
    "                # which directories (after mount pattern) are within that mount\n",
    "                if pattern:\n",
    "                    pattern_glob = Path(pattern.group().strip()).glob(after_pattern) \n",
    "                else:\n",
    "                    continue\n",
    "                # after mount pattern could retrieve multiple locations\n",
    "                if pattern_glob: \n",
    "                    not_tmp_locations += [str(pg) for pg in pattern_glob]\n",
    "    \n",
    "    return tmp, not_tmp_locations\n",
    "\n",
    "\n",
    "def get_free_disk(location):\n",
    "    \"\"\"\n",
    "    Return location's free disk space in gb.\n",
    "    \"\"\"\n",
    "    \n",
    "    _total, _used, free = shutil.disk_usage(location)\n",
    "    # transform to gb\n",
    "    return free // (2**30)\n",
    "\n",
    "\n",
    "def get_used_disk(location):\n",
    "    \"\"\"\n",
    "    Return location's used disk space in gb.\n",
    "    \"\"\"\n",
    "    \n",
    "    _total, used, _free = shutil.disk_usage(location)\n",
    "    # transform to gb\n",
    "    return used // (2**30)\n",
    "\n",
    "\n",
    "def get_available_disk_resource(location, host, status_csv):\n",
    "    \"\"\"\n",
    "    Return available disk space available (in gb).\n",
    "    \"\"\"\n",
    "    \n",
    "    total_req_disk_others_gb = (pd.read_csv(status_csv)\n",
    "    .query(\"location == @location and status == 'ongoing' and host == @host\")\n",
    "    .assign(\n",
    "        used_disk_gb = lambda df_: \n",
    "            df_['location'].apply(lambda x_: get_used_disk(x_)),\n",
    "        req_disk_gb = lambda df_: \n",
    "            (df_['req_disk_gb'] - df_['used_disk_gb'])\n",
    "    )\n",
    "    .assign(\n",
    "        req_disk_gb = lambda df_: \n",
    "            df_['req_disk_gb'].mask(df_['req_disk_gb'] < 0, 0)\n",
    "    )\n",
    "    ['req_disk_gb']\n",
    "    .sum()\n",
    "    )\n",
    "        \n",
    "    current_free_gb = get_free_disk(location)\n",
    "    \n",
    "    return current_free_gb - total_req_disk_others_gb\n",
    "\n",
    "\n",
    "def set_status(status_csv, job_name, job_id, req_disk_gb, host, location, job_dir, status, start):\n",
    "    \"\"\"\n",
    "    Add a new job status.\n",
    "    \"\"\"\n",
    "    \n",
    "    status_df = pd.read_csv(status_csv)\n",
    "    \n",
    "    new_status = {\n",
    "        'job_name':[job_name],\n",
    "        'job_id':[job_id],\n",
    "        'req_disk_gb':[req_disk_gb],\n",
    "        'host':[host],\n",
    "        'location':[location],\n",
    "        'job_dir':[job_dir],\n",
    "        'status':[status],\n",
    "        'start':[start],\n",
    "        'update':[None],\n",
    "        'traceback':[None]\n",
    "        }\n",
    "    \n",
    "    new_status = pd.DataFrame(new_status)\n",
    "    \n",
    "    status_df = pd.concat([status_df, new_status])\n",
    "    \n",
    "    status_df.to_csv(status_csv, index=False)\n",
    "    \n",
    "    return status_df\n",
    "\n",
    "\n",
    "def update_status(status_csv, job_name, job_id, host, location, status, update, traceback=None):\n",
    "    \"\"\"\n",
    "    Update an existing job status.\n",
    "    \"\"\"\n",
    "    \n",
    "    status_df = pd.read_csv(status_csv)\n",
    "    \n",
    "    is_job = (\n",
    "    (status_df['job_name'] == job_name) &\n",
    "    (status_df['job_id'] == job_id) &\n",
    "    (status_df['host'] == host) &\n",
    "    (status_df['location'] == location) \n",
    "    )\n",
    "    \n",
    "    status_df = (status_df\n",
    "    .assign(\n",
    "        status = lambda df_: df_['status'].mask(is_job, status),\n",
    "        update = lambda df_: df_['update'].mask(is_job, update),\n",
    "        traceback = lambda df_: df_['traceback'].mask(is_job, traceback)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    status_df.to_csv(status_csv, index=False)\n",
    "    \n",
    "    return status_df\n",
    "\n",
    "\n",
    "# Functions for cloning and checking out\n",
    "def do_dead_annex(dpath='cwd'):\n",
    "    \"\"\"\n",
    "    Set cwd as dead annex or submodules as dead annex.\n",
    "    \"\"\"\n",
    "    if dpath == 'cwd':\n",
    "        cmd = ['git', 'annex', 'dead', 'here']\n",
    "    else: \n",
    "        cmd = ['git', 'submodule', 'foreach', '--recursive', 'git', 'annex', 'dead', 'here']\n",
    "    subprocess.run(cmd)\n",
    "    \n",
    "\n",
    "def do_checkout(job_name, dpath='cwd'):\n",
    "    \"\"\"\n",
    "    Change to a job branch.\n",
    "    \"\"\"\n",
    "    if dpath == 'cwd':\n",
    "        cmd = ['git', 'checkout', '-b', job_name]\n",
    "    else:\n",
    "        cmd = ['git', '-C', dpath ,'checkout', '-b', job_name]\n",
    "    \n",
    "    subprocess.run(cmd)\n",
    "\n",
    "    \n",
    "def get_private_subdataset(clone_target, sd_path, sd_id):\n",
    "    # Assume clone_target is a RIA store\n",
    "    clone_path = str(Path(clone_target) / Path(sd_id[:3]) / Path(sd_id[3:]))\n",
    "    \n",
    "    git_clone_command = ['git', 'clone', clone_path, sd_path]\n",
    "    subprocess.run(git_clone_command)\n",
    "    \n",
    "    git_config_annex_private = ['git', '-C', sd_path, 'config', 'annex.private', 'true']\n",
    "    subprocess.run(git_config_annex_private)\n",
    "    \n",
    "    git_annex_init = ['git', '-C', sd_path, 'annex', 'init']\n",
    "    subprocess.run(git_annex_init)\n",
    "\n",
    "\n",
    "def git_add_remote(push_path, dpath='cwd'):\n",
    "    if dpath == 'cwd':\n",
    "        cmd = ['git', 'remote', 'add', 'outputstore', push_path]\n",
    "    else:\n",
    "        cmd = ['git', '-C', dpath, 'remote', 'add', 'outputstore', push_path]\n",
    "        \n",
    "    subprocess.run(cmd)\n",
    "\n",
    "def git_push(dpath='cwd'):\n",
    "    if dpath == 'cwd':\n",
    "        cmd = ['git', 'push', 'outputstore']\n",
    "    else:\n",
    "        cmd = ['git', '-C', dpath, 'push', 'outputstore']\n",
    "    \n",
    "    subprocess.run(cmd)\n",
    "\n",
    "\n",
    "# cleanup and exception handling\n",
    "def cleanup(job_dir):\n",
    "    subprocess.run(['chmod', '-R', '+w', job_dir])\n",
    "    subprocess.run(['rm', '-rf', job_dir])\n",
    "\n",
    "\n",
    "# def excepthook(exctype, value, tb):\n",
    "#     with status_lock:\n",
    "#         update_status(status_csv, job_name, job_id, host, location, status='error', traceback=tb)\n",
    "#     print('Type:', exctype)\n",
    "#     print('Value:', value)\n",
    "#     print('Traceback:', tb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp, not_tmp_locations = get_locations(ephemeral_locations)\n",
    "\n",
    "# manage available disk space\n",
    "if req_disk_gb is None:\n",
    "    req_disk_gb = 0\n",
    "    \n",
    "with status_lock:\n",
    "    \n",
    "    found_location=False\n",
    "    \n",
    "    if tmp:\n",
    "        tmp = '/tmp'\n",
    "        available_disk = get_available_disk_resource(tmp, host, status_csv)\n",
    "        if req_disk_gb < available_disk:\n",
    "            found_location=True\n",
    "            location=tmp\n",
    "            \n",
    "    \n",
    "    elif not_tmp_locations and not found_location:\n",
    "        not_tmp_df = (\n",
    "            pd.DataFrame({'location':not_tmp_locations})\n",
    "            .assign(available_disk = lambda df_: \n",
    "                df_['location'].apply(lambda x_: get_available_disk_resource(x_, host, status_csv))\n",
    "                )\n",
    "            .sort_values('free_space', ascending=False)\n",
    "            )\n",
    "\n",
    "        if req_disk_gb < not_tmp_df['available_disk'].iat[0]:\n",
    "            found_location = True\n",
    "            location = not_tmp_df['location'].iat[0]\n",
    "            \n",
    "            \n",
    "    if found_location:\n",
    "        job_dir = str(Path(location) / f'job-{job_name}-{user}')\n",
    "        set_status(status_csv, job_name, job_id, req_disk_gb, host, location, job_dir, status='ongoing', start=datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "    else:\n",
    "        set_status(status_csv, job_name, job_id, req_disk_gb, host, location, job_dir=None, status='no-space', start=datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "        raise Exception(\"Coulnd't find a place with enough disk space.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    clone_ria_prefix = re.search(r'ria\\+\\w+:\\/{2}', clone_target).group()\n",
    "    clone_target = clone_target.replace(clone_ria_prefix, '')\n",
    "except:\n",
    "    # assume ria requires a file protocol if no protocol in the job_config\n",
    "    clone_ria_prefix = 'ria+file://'\n",
    "try:\n",
    "    push_target = re.sub(r'ria\\+\\w+:\\/{2}', '', push_target)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Attempting a clone into /tmp/job-sub-001A_bet-ramirezd \n",
      "[INFO] Attempting to clone from file:///misc/geminis2/ramirezd/test_bet/.fairlybig/input_ria/64f/1d8ac-346d-4a0a-8c5c-8dca7e25ef7c to /tmp/job-sub-001A_bet-ramirezd \n",
      "[INFO] Completed clone attempts for Dataset(/tmp/job-sub-001A_bet-ramirezd) \n",
      "[INFO] Reconfigured input_ria-storage for ria+file:///misc/geminis2/ramirezd/test_bet/.fairlybig/input_ria/ \n",
      "[INFO] Configure additional publication dependency on \"input_ria-storage\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configure-sibling(ok): . (sibling)\n",
      "install(ok): /tmp/job-sub-001A_bet-ramirezd (dataset)\n",
      "action summary:\n",
      "  configure-sibling (ok: 1)\n",
      "  install (ok: 1)\n",
      "subdataset(ok): inputs/containers (dataset)\n",
      "subdataset(ok): inputs/mri-raw (dataset)\n",
      "subdataset(ok): outputs (dataset)\n"
     ]
    }
   ],
   "source": [
    "super_clone_target = f'{clone_ria_prefix}{clone_target}#{super_ds_id}'\n",
    "\n",
    "dl.clone(source=super_clone_target, path=job_dir, git_clone_opts=['-c annex.private=true'])\n",
    "os.chdir(job_dir)\n",
    "\n",
    "push_path = str(Path(push_target) / Path(super_ds_id[:3]) / Path(super_ds_id[3:]))\n",
    "git_add_remote(push_path, 'cwd')\n",
    "\n",
    "ds = dl.Dataset(job_dir)\n",
    "sd = pd.DataFrame(ds.subdatasets())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_datasets is None:\n",
    "    output_datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_datasets and not (pd.Series(output_datasets).isin(sd['gitmodule_name']).all()):\n",
    "    raise Exception(\"Not all output datasets are found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'outputs'...\n",
      "done.\n",
      "  Remote origin: This repository is not initialized for use by git-annex, but /misc/geminis2/ramirezd/test_bet/.fairlybig/input_ria/3f9/fea76-50a0-489a-b9f5-27bd92a17e09/annex/objects/ exists, which indicates this repository was used by git-annex before, and may have lost its annex.uuid and annex.version configs. Either set back missing configs, or run git-annex init to initialize with a new uuid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init  \n",
      "(Auto enabling special remote output_ria-storage...)\n",
      "(Auto enabling special remote input_ria-storage...)\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "for output_dataset in output_datasets:\n",
    "    sd_id = sd.query(\"gitmodule_name == @output_dataset\")['gitmodule_datalad-id'].iat[0]\n",
    "    get_private_subdataset(clone_target, output_dataset, sd_id)\n",
    "    \n",
    "    push_path = str(Path(push_target) / Path(sd_id[:3]) / Path(sd_id[3:]))\n",
    "    git_add_remote(push_path, output_dataset)\n",
    "    \n",
    "if not Path('outputs').exists():\n",
    "    Path('outputs').mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Switched to a new branch 'job-sub-001A_bet'\n",
      "Switched to a new branch 'job-sub-001A_bet'\n"
     ]
    }
   ],
   "source": [
    "# Checkout to job branch\n",
    "branch_name = f'job-{job_name}'\n",
    "\n",
    "for output_dataset in output_datasets:\n",
    "    do_checkout(branch_name, output_dataset)\n",
    "    \n",
    "do_checkout(branch_name, 'cwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preget_inputs is None or None in preget_inputs:\n",
    "    preget_inputs = []\n",
    "for preget_input in preget_inputs:\n",
    "    dl.get(preget_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/inb/soporte/lanirem_software/go_1.20.6/bin:/home/inb/soporte/lanirem_software/apptainer/bin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/inb/ramirezd/.local/bin:/home/inb/soporte/lanirem_software/ANTs_2.4.4/Scripts:/home/inb/soporte/lanirem_software/ANTs_2.4.4/bin:/home/inb/soporte/lanirem_software/fsl_6.0.7.4/share/fsl/bin:/misc/geminis/ramirezd/miniconda3/envs/py-minis/bin:/misc/geminis/ramirezd/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/inb/soporte/inb_tools:/home/inb/ramirezd/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/misc/geminis2/hcp_workbench/bin_linux64:/misc/geminis2/hcp_workbench/bin_linux64:/misc/geminis2/hcp_workbench/bin_linux64\n"
     ]
    }
   ],
   "source": [
    "%set_env PATH=/home/inb/soporte/lanirem_software/go_1.20.6/bin:/home/inb/soporte/lanirem_software/apptainer/bin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/inb/ramirezd/.local/bin:/home/inb/soporte/lanirem_software/ANTs_2.4.4/Scripts:/home/inb/soporte/lanirem_software/ANTs_2.4.4/bin:/home/inb/soporte/lanirem_software/fsl_6.0.7.4/share/fsl/bin:/misc/geminis/ramirezd/miniconda3/envs/py-minis/bin:/misc/geminis/ramirezd/miniconda3/condabin:/opt/sge/bin:/opt/sge/bin/lx-amd64:/home/inb/soporte/inb_tools:/home/inb/ramirezd/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/misc/geminis2/hcp_workbench/bin_linux64:/misc/geminis2/hcp_workbench/bin_linux64:/misc/geminis2/hcp_workbench/bin_linux64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Making sure inputs are available (this may take some time) \n",
      "[INFO] Attempting a clone into /tmp/job-sub-001A_bet-ramirezd/inputs/mri-raw \n",
      "[INFO] Attempting to clone from file:///misc/geminis2/ramirezd/test_bet/.fairlybig/input_ria/e5c/d662d-42d9-4f28-b6ec-1b54906a0015 to /tmp/job-sub-001A_bet-ramirezd/inputs/mri-raw \n",
      "[INFO] Attempting to clone from /misc/geminis2/twinsmx/datasets/mri_study/mri_study-raw/mri-raw to /tmp/job-sub-001A_bet-ramirezd/inputs/mri-raw \n",
      "[INFO] Completed clone attempts for Dataset(/tmp/job-sub-001A_bet-ramirezd/inputs/mri-raw) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get(ok): inputs/mri-raw/sub-001A/anat/sub-001A_T1w.nii.gz (file) [from origin...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Attempting a clone into /tmp/job-sub-001A_bet-ramirezd/inputs/containers \n",
      "[INFO] Attempting to clone from file:///misc/geminis2/ramirezd/test_bet/.fairlybig/input_ria/8b1/4b026-dc93-4d34-84dc-7e459c08be13 to /tmp/job-sub-001A_bet-ramirezd/inputs/containers \n",
      "[INFO] Attempting to clone from /misc/geminis2/containers to /tmp/job-sub-001A_bet-ramirezd/inputs/containers \n",
      "[INFO] Completed clone attempts for Dataset(/tmp/job-sub-001A_bet-ramirezd/inputs/containers) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get(ok): inputs/containers/.datalad/environments/fsl-6-0-4/image (file) [from origin...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] == Command start (output follows) ===== \n",
      "[INFO] == Command exit (modification check follows) ===== \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run(ok): /tmp/job-sub-001A_bet-ramirezd (dataset) [apptainer run -e inputs/containers/.data...]\n",
      "add(ok): sub-001A_T1w_bet.nii.gz (file)\n",
      "save(ok): outputs (dataset)\n",
      "add(ok): outputs (dataset)\n",
      "add(ok): .gitmodules (file)\n",
      "save(ok): . (dataset)\n",
      "action summary:\n",
      "  add (ok: 3)\n",
      "  get (notneeded: 3, ok: 2)\n",
      "  run (ok: 1)\n",
      "  save (notneeded: 2, ok: 2)\n"
     ]
    }
   ],
   "source": [
    "if message is None:\n",
    "    message = branch_name\n",
    "\n",
    "if commit:\n",
    "    dl.rerun(\n",
    "        revision=commit,\n",
    "        explicit=is_explicit\n",
    "    )\n",
    "    \n",
    "elif container:\n",
    "    dl.containers_run(\n",
    "        dl_cmd,\n",
    "        container_name=container,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        message=message,\n",
    "        explicit=is_explicit\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    dl.run(\n",
    "        dl_cmd,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        message=message,\n",
    "        explicit=is_explicit\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Determine push target \n",
      "[INFO] Push refspecs \n",
      "[INFO] Transfer data \n",
      "[INFO] Finished push of Dataset(/tmp/job-sub-001A_bet-ramirezd) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action summary:\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Determine push target \n",
      "[INFO] Push refspecs \n",
      "[INFO] Transfer data \n",
      "[INFO] Finished push of Dataset(/tmp/job-sub-001A_bet-ramirezd) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action summary:\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To /misc/geminis2/ramirezd/test_bet/.fairlybig/output_ria/64f/1d8ac-346d-4a0a-8c5c-8dca7e25ef7c\n",
      " * [new branch]      job-sub-001A_bet -> job-sub-001A_bet\n",
      "To /misc/geminis2/ramirezd/test_bet/.fairlybig/output_ria/3f9/fea76-50a0-489a-b9f5-27bd92a17e09\n",
      " * [new branch]      job-sub-001A_bet -> job-sub-001A_bet\n"
     ]
    }
   ],
   "source": [
    "# push annex data\n",
    "dl.push(\n",
    "    dataset='.',\n",
    "    to='output_ria-storage',\n",
    ")\n",
    "\n",
    "for output_dataset in output_datasets:\n",
    "    dl.push(\n",
    "        dataset=output_dataset,\n",
    "        to='output_ria-storage',\n",
    "    )\n",
    "\n",
    "# push git data\n",
    "with push_lock:\n",
    "    git_push('cwd')\n",
    "    for output_dataset in output_datasets:\n",
    "        git_push(output_dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup(job_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed succesfully.\n"
     ]
    }
   ],
   "source": [
    "with status_lock:\n",
    "    \n",
    "    update_status(status_csv, \n",
    "                    job_name, \n",
    "                    job_id, \n",
    "                    host, \n",
    "                    location, \n",
    "                    status='completed', \n",
    "                    update=datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\"), \n",
    "                    traceback=None\n",
    "                    )\n",
    "\n",
    "print(\"Job completed succesfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-minis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7ca23d180daa64c5df23970c68613774f6face07a17b610016dc57dc2566e45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
